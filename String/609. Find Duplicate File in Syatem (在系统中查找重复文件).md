# 609. [Find Duplicate File in Syatem][FDFS] (在系统中查找重复文件)

[FDFS]: https://leetcode-cn.com/problems/find-duplicate-file-in-system/

**Difficulty:** `medium`

**Tag:** `Hash Table`/ `String`

------

## Question

Given  a list of directory info including directory path, and all the files with contents in this directory, you need to find out all the groups of duplicate files in the file system in terms of their paths.

A group of duplicate files consists of at least **two** files that have exactly the same content.

A single directory info string in the **input** list has the following format:

```
"root/d1/d2/.../dm f1.txt(f1_content) f2.txt(f2_content) ... fn.txt(fn_content)"
```

It means there are **n** files (`f1.txt`, `f2.txt` ... `fn.txt` with content `f1_content`, `f2_content` ... `fn_content`, respectively) in directory `root/d1/d2/.../dm`. Note that n >= 1 and m >= 0. If m = 0, it means the directory is just the root directory.

The **output** is a list of group of duplicate file paths. For each group, it contains all the file paths of the files that have the same content. A file path is a string that has the following format:

```
"directory_path/file_name.txt"
```

**Example 1:**

```
Input:
["root/a 1.txt(abcd) 2.txt(efgh)", "root/c 3.txt(abcd)", "root/c/d 4.txt(efgh)", "root 4.txt(efgh)"]
Output:  
[["root/a/2.txt","root/c/d/4.txt","root/4.txt"],["root/a/1.txt","root/c/3.txt"]]
```

------

## Solution

#### Approach 1 -- Brute Force [Time Limit Exceeded]

For the brute force solution, firstly we obtain the directory paths, the filenames and file contents separately by appropriately splitting the elements of the `paths` list. While doing so, we keep on creating a `list` which contains the full path of every file along with the contents of the file. The `list` contains data in the form:

`[[file1_full_path, file1_contents], [file2_full_path, file2_contents],..., [filen_full_path, filen_contents]]`

Once this is done, we iterate over this list. For every element *i* chosen from the list, we iterate over the whole `list` to find another element *j* whose file contents are the same as the *i*th element. For every such element found, we put the *j*th element's file path in a temporary list `l` and we also mark the *j*th element as visited so that this element isn't considered again in the future. Thus, when we reach the end of the array for every *i*th element, we obtain a list of file paths in `l`, which have the same contents as the file corresponding to the *i*th element. If this list isn't empty, it indicates that there exists content duplicate to the *i*th element. Thus, we also need to put *i*th element's file path in the `l`.

At the end of each iteration, we put this list `l` obtained in the resultant list `res` and reset the list `l` for finding the duplicates of the next element.

##### Reference Code

```java
public class Solution {
    public List < List < String >> findDuplicate(String[] paths) {
        List < String[] > list = new ArrayList < > ();
        for (String path: paths) {
            String[] values = path.split(" ");
            for (int i = 1; i < values.length; i++) {
                String[] name_cont = values[i].split("\\(");
                name_cont[1] = name_cont[1].replace(")", "");
                list.add(new String[] {
                    values[0] + "/" + name_cont[0], name_cont[1]
                });
            }
        }
        boolean[] visited = new boolean[list.size()];
        List < List < String >> res = new ArrayList < > ();
        for (int i = 0; i < list.size() - 1; i++) {
            if (visited[i])
                continue;
            List < String > l = new ArrayList < > ();
            for (int j = i + 1; j < list.size(); j++) {
                if (list.get(i)[1].equals(list.get(j)[1])) {
                    l.add(list.get(j)[0]);
                    visited[j] = true;
                }
            }
            if (l.size() > 0) {
                l.add(list.get(i)[0]);
                res.add(l);
            }
        }
        return res;
    }
}
```

#### Approach 2 -- Using HashMap

In this approach, firstly we obtain the directory paths, the file names and their contents separately by appropriately splitting each string in the given `paths` list. In order to find the files with duplicate contents, we make use of a HashMap `map`, which stores the data in the form:`(contents, list of file paths with this content)`. Thus, for every file's contents, we check if the same content already exist in the hashmap. If so, we add the current file's path to the list of files corresponding to the current contents. Otherwise, we create a new entry in the `map`, with the current contents as the key and the value being a list with only one entry (the current file's path).

At the end, we find out the contents corresponding to which at least two file paths exist. We obtain the result list `res`, which is a list of lists contains these file paths corresponding to the same contents.

##### Reference Code

```java

public class Solution {
    public List < List < String >> findDuplicate(String[] paths) {
        HashMap < String, List < String >> map = new HashMap < > ();
        for (String path: paths) {
            String[] values = path.split(" ");
            for (int i = 1; i < values.length; i++) {
                String[] name_cont = values[i].split("\\(");
                name_cont[1] = name_cont[1].replace(")", "");
                List < String > list = map.getOrDefault(name_cont[1], new ArrayList < String > ());
                list.add(values[0] + "/" + name_cont[0]);
                map.put(name_cont[1], list);
            }
        }
        List < List < String >> res = new ArrayList < > ();
        for (String key: map.keySet()) {
            if (map.get(key).size() > 1)
                res.add(map.get(key));
        }
        return res;
    }
}
```

**Complexity:** `Time O(N*X)` / `Space O(N*X)` *n* strings of average length *x* is parsed.

##### Own Code

```java
class Solution {
    public List<List<String>> findDuplicate(String[] paths) {
        Map<String, List<String>> contents = new HashMap<String, List<String>>();
        Map<String, List<String>> res = new HashMap<String, List<String>>();
        
        for (String path : paths) {
            int i = 0;
            //标记路径
            while (path.charAt(i) != ' ') {
                i++;
            }
            String filePath = path.substring(0, i);
            //int j = i + 1;
            
            //获取文件名，注意可能有多个文件
            for (int j = i + 1; j < path.length();){
                while (path.charAt(j) != '(') {
                    j++;
                }
                String fileName = path.substring(i + 1, j);
            
                int k = j + 1;
            
            //获取文件内容
                while (path.charAt(k) != ')') {
                    k++;
                }
                String fileContent = path.substring(j + 1, k);
            
                String str = filePath + "/" + fileName;
                //判断文件内容是否重复
                if (contents.containsKey(fileContent)) {
                    //重复，则将文件路径+/+文件名加入列表
                    contents.get(fileContent).add(str);
                    res.put(fileContent, contents.get(fileContent));
                }else {
                    contents.put(fileContent, new ArrayList<String>(Arrays.asList(str)));
                }
                
                i = k + 1;
                j = i + 1;
            }  
        }   
        return new ArrayList<List<String>>(res.values());
    }
}
```

